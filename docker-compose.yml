services:
  app:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - 8005:8005
      - 5678:5678
    volumes:
      - .:/code
    command: ["uvicorn", "src.main:app", "--host", "0.0.0.0", "--port", "8005", "--reload"]
    restart: always
    depends_on:
      - ollama
      - ollama-webui
    networks:
      - ollama-docker

  ollama:
    image: ollama/ollama:latest
    ports:
      - 11435:11434
    volumes:
      - ./ollama/ollama:/root/.ollama
    networks:
      - ollama-docker
    container_name: ollama
    pull_policy: always
    tty: true
    restart: unless-stopped
    environment:
      - OLLAMA_KEEP_ALIVE=24h
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]


  ollama-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: ollama-webui
    volumes:
      - ./ollama/ollama-webui:/app/backend/data
    depends_on:
      - ollama
    ports:
      - 8082:8080
    environment:
      - USE_OLLAMA_DOCKER=true
      - OLLAMA_BASE_URLS=http://localhost:11435
      - ENV=dev
      - WEBUI_AUTH=false
      - WEBUI_NAME=4th Creative Party <AI>
      - WEBUI_URL=http://localhost:8082
      - WEBUI_SECRET_KEY=twotwo95
    extra_hosts:
      - localhost:host-gateway
    restart: unless-stopped
    networks:
      - ollama-docker


networks:
  ollama-docker:
    external: false
